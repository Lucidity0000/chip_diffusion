{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../diffusion\")\n",
    "\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import pickle\n",
    "import collections\n",
    "import re\n",
    "\n",
    "DEVICE=\"cuda:0\"\n",
    "\n",
    "def plot_set(data_set, nrows = 3, ncols = 6, scale_factor = 1.0):\n",
    "    fig, axes = plt.subplots(nrows, ncols)\n",
    "    fig.set_size_inches(scale_factor*ncols*2, scale_factor*nrows*2)\n",
    "    for i, (x, cond) in enumerate(data_set):\n",
    "        img = utils.visualize_placement(x, cond)\n",
    "        ax = axes[i//ncols][i%ncols] if nrows > 1 else axes[i%ncols]\n",
    "        ax.imshow(img)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.show()\n",
    "\n",
    "def preprocess_placement(x, cond, scale=1):\n",
    "    if len(cond.chip_size) == 4: # chip_size is [x_start, y_start, x_end, y_end]\n",
    "        chip_size = (cond.chip_size[2] - cond.chip_size[0], cond.chip_size[3] - cond.chip_size[1])\n",
    "        chip_offset = (cond.chip_size[0], cond.chip_size[1])\n",
    "    else:\n",
    "        chip_size = (cond.chip_size[0], cond.chip_size[1])\n",
    "        chip_offset = (0, 0)\n",
    "    chip_size = torch.tensor(chip_size, dtype = torch.float32).view(1, 2)\n",
    "    chip_offset = torch.tensor(chip_offset, dtype = torch.float32).view(1, 2)\n",
    "    x = (torch.tensor(x, dtype=torch.float32) - chip_offset)/scale\n",
    "    x = 2 * (x / chip_size) - 1\n",
    "    # use center of instance as coordinate point and reference for terminal\n",
    "    x = x + cond.x/2\n",
    "    return x\n",
    "\n",
    "def eval_set(data_set, eval_fn, **kwargs):\n",
    "    return [eval_fn(x, cond, normalized_hpwl=False, **kwargs)[1] for (x, cond) in data_set]\n",
    "\n",
    "def plot_and_eval_set(data_set, eval_fn, nrows = 3, ncols = 6, scale_factor = 1.0, title=None):\n",
    "    fig, axes = plt.subplots(nrows, ncols)\n",
    "    fig.set_size_inches(scale_factor*ncols*2, scale_factor*nrows*2 + 0.2)\n",
    "    if title:\n",
    "        fig.suptitle(title)\n",
    "    for i, (x, cond) in enumerate(data_set):\n",
    "        img = utils.visualize_placement(x, cond)\n",
    "        ax = axes[i//ncols][i%ncols] if nrows > 1 else axes[i%ncols]\n",
    "        ax.imshow(img)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        metric = eval_fn(x, cond, normalized_hpwl=False)[1]\n",
    "        ax.set_title(f\"{metric:.0f}\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_and_print_metrics(metrics, images, nrows = 3, ncols = 6, scale_factor = 1.0, title=None):\n",
    "    fig, axes = plt.subplots(nrows, ncols)\n",
    "    fig.set_size_inches(scale_factor*ncols*2, scale_factor*nrows*2 + 0.2)\n",
    "    if title:\n",
    "        fig.suptitle(title)\n",
    "    for i, (metric, image) in enumerate(zip(metrics, images)):\n",
    "        ax = axes[i//ncols][i%ncols] if nrows > 1 else axes[i%ncols]\n",
    "        ax.imshow(image.T, origin=\"lower\", cmap=\"gray_r\")\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(f\"{metric:.5g}\")\n",
    "    plt.show()\n",
    "\n",
    "def from_pl(base_path, cond, circuit_name=\"adaptec1\"):\n",
    "    # converts placements to a pl output, produces data as torch (V, 2) tensor\n",
    "    V = cond.num_nodes\n",
    "    data = torch.zeros((V,2), dtype=torch.float32)\n",
    "    fixed_mask = torch.zeros((V,), dtype=bool)\n",
    "    original_pl_file = os.path.join(base_path, circuit_name, f\"{circuit_name}_placed.pl\")\n",
    "    with open(original_pl_file, \"r\") as f:\n",
    "        i = 0\n",
    "        for line in f.readlines():\n",
    "            line_stripped = line.strip()\n",
    "            if not line_stripped.startswith(\"o\") and not line_stripped.startswith(\"p\"):\n",
    "                continue # not relevant to placement\n",
    "            else:\n",
    "                line_split = line_stripped.split()\n",
    "                obj_name = line_split[0]\n",
    "                obj_index = cond.name_index_mapping[obj_name]\n",
    "                data[obj_index, 0] = float(line_split[1]) \n",
    "                data[obj_index, 1] = float(line_split[2])\n",
    "                if len(line_split) > 5:\n",
    "                    if \"/FIXED\" in line_stripped:\n",
    "                        fixed_mask[obj_index] = True\n",
    "                    else:\n",
    "                        fixed_mask[obj_index] = False\n",
    "                else:\n",
    "                    if \"/FIXED\" in line_stripped: # unexpected\n",
    "                        import ipdb; ipdb.set_trace()\n",
    "                    fixed_mask[obj_index] = False\n",
    "                i += 1\n",
    "    data = data / 1000 # TRICK FOR ISPD\n",
    "    return data, fixed_mask\n",
    "\n",
    "def load_ispd_placements(base_dir, graph_dataset):\n",
    "    # Generates dataset of (x, cond) using placements from base_dir and cond from graph_dataset\n",
    "    ispd_mapping = [f\"adaptec{i+1}\" for i in range(4)] + [f\"bigblue{i+1}\" for i in range(4)] # maps file_idx to placement name\n",
    "    out_set = []\n",
    "    for _, cond in graph_dataset:\n",
    "        circuit_name = ispd_mapping[cond.file_idx]\n",
    "        load_path = os.path.join(base_dir, circuit_name, f\"{circuit_name}_placed.pl\")\n",
    "        if not os.path.exists(load_path):\n",
    "            print(f\"skipping {circuit_name}, {load_path} not found\")\n",
    "            continue\n",
    "        placement, fixed_mask = from_pl(base_dir, cond=cond, circuit_name=circuit_name)\n",
    "        cond[\"fixed_mask\"] = fixed_mask\n",
    "        placement = preprocess_placement(placement, cond)\n",
    "        out_set.append((placement, cond))\n",
    "    return out_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISPD_CHIP_SIZES = {\n",
    "    0: [0.459, 0.459, 0.459 + 10692/1000, 0.459 + 12*890/1000], # adaptec1\n",
    "    1: [0.609, 0.616, 0.609 + 14054/1000, 0.616 + 12*1170/1000],\n",
    "    2: [0.036, 0.058, 0.036 + 23190/1000, 23386/1000],\n",
    "    3: [0.036, 0.058, 0.036 + 23190/1000, 23386/1000],\n",
    "    4: [0.459, 0.459, 0.459 + 10692/1000, 11139/1000], # bigblue1 \n",
    "    5: [0.036, 0.076, 0.036 + 18690/1000, 18868/1000],\n",
    "    6: [0.036, 0.076, 0.036 + 27690/1000, 27868/1000],\n",
    "    7: [0.036, 0.058, 0.036 + 32190/1000, 32386/1000],\n",
    "}\n",
    "\n",
    "class Pin():\n",
    "    def __init__(self, id, obj_name, offset):\n",
    "        self.obj_name = obj_name\n",
    "        self.id = id\n",
    "        self.offset = offset\n",
    "    def __str__(self):\n",
    "        return f\"Pin(id:{self.id}, obj:{self.obj_name}, {self.offset})\"\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "def flip_stack(data):\n",
    "    \"\"\"\n",
    "    data is tensor (B, N, ...)\n",
    "    flips along N axis, then appends along B axis\n",
    "    Output is shaped (2B, N, ...)\n",
    "    \"\"\"\n",
    "    return torch.concatenate((data, torch.flip(data, dims=(1,))), dim=0)\n",
    "\n",
    "def parse_bookshelf(nodes_path, nets_path, pl_path, verbose=False, warn=False):\n",
    "    \"\"\"\n",
    "    Parses bookshelf circuit and placement and generates (x, cond) python objects\n",
    "\n",
    "    Returns:\n",
    "    - x: (V, 2) tensor with placement of objects (measured to bottom left) in non-normalized units\n",
    "    - cond: torch_geometric Data object containing the following keys:\n",
    "        - x: (V, 2) tensor containing sizes of objects\n",
    "        - is_ports: (V,) bool tensor\n",
    "        - is_macros: (V,) bool tensor\n",
    "        - edge_index: (2, E) int64 tensor\n",
    "        - edge_attr: (E, 4) tensor with pin offsets for start and end pins, measured from bottom left of object\n",
    "        - edge_pin_id: (E, 2) locally unique pin id (each pin has id unique among pins on the same object)\n",
    "        - name_index_mapping: {str : int} maps object names to index in tensors\n",
    "        NOTE: chip_size needs to be specified externally\n",
    "    \"\"\"\n",
    "\n",
    "    SCALING_UNITS = 1000\n",
    "    print_fn = print if verbose else lambda x: 0\n",
    "    warn_fn = print if warn else lambda x: 0\n",
    "\n",
    "    # Ingest Placement file\n",
    "    with open(pl_path, 'r') as pl_file:\n",
    "        pl_file_data = pl_file.readlines()\n",
    "    placement_line_pattern = re.compile(r\"^\\s*(\\S+)\\s+(\\d+(?:\\.\\d*)?)\\s+(\\d+(?:\\.\\d+)?)\\s*:\\s*(F?N|S|E|W)\\s*(/FIXED)?\\s*$\")\n",
    "    placement_dict = {\"obj_names\": [], \"positions\": [], \"is_fixed\": []}\n",
    "    for pl_line in pl_file_data:\n",
    "        match = re.match(placement_line_pattern, pl_line)\n",
    "        if match: # valid placement line\n",
    "            placement_dict[\"obj_names\"].append(match[1])\n",
    "            placement_dict[\"positions\"].append((float(match[2]), float(match[3])))\n",
    "            assert match[4] == \"N\", \"non-North orientations not supported\"\n",
    "            placement_dict[\"is_fixed\"].append(match[5] is not None) # unused for ISPD\n",
    "    print_fn(\"num objects:\", len(placement_dict[\"positions\"]))\n",
    "\n",
    "    # Ingest Nodes file\n",
    "    with open(nodes_path, 'r') as nodes_file:\n",
    "        nodes_file_data = nodes_file.readlines()\n",
    "    node_line_pattern = re.compile(r\"^\\s*(\\S+)\\s+(\\d+(?:\\.\\d+)?)\\s+(\\d+(?:\\.\\d*)?)\\s+(terminal)?$\")\n",
    "    node_size_dict = {} # name: size\n",
    "    node_macro_dict = {} # name: is_macro\n",
    "    for node_line in nodes_file_data:\n",
    "        match = re.match(node_line_pattern, node_line)\n",
    "        if match: # valid node data line\n",
    "            node_name = match[1]\n",
    "            assert node_name not in node_size_dict, \"duplicate node names detected\"\n",
    "            \n",
    "            node_size_dict[node_name] = (float(match[2]), float(match[3]))\n",
    "            node_macro_dict[node_name] = (match[4] is not None)\n",
    "    print_fn(\"num unique objects:\", len(node_size_dict))\n",
    "    print_fn(\"num macros:\", sum(node_macro_dict.values()))\n",
    "\n",
    "    # Ingest Nets file\n",
    "    with open(nets_path, 'r') as nets_file:\n",
    "        nets_file_data = nets_file.readlines()\n",
    "    \n",
    "    # header for each net\n",
    "    net_header_pattern = re.compile(r\"^\\s*NetDegree\\s*:\\s*(\\d+)\\s+(\\S+)\\s*$\") \n",
    "    # pin data in a net\n",
    "    net_line_pattern = re.compile(r\"^\\s*(\\S+)\\s+(I|O)\\s*:\\s*(-?\\d+(?:\\.\\d+)?)\\s+(-?\\d+(?:\\.\\d+)?)\\s*$\")\n",
    "    \n",
    "    nets_started = False\n",
    "    line_num = 0\n",
    "    nets = []\n",
    "    one_deg_nets = 0\n",
    "    while line_num < len(nets_file_data):\n",
    "        nets_line = nets_file_data[line_num]\n",
    "        header_match = re.match(net_header_pattern, nets_line)\n",
    "        line_num += 1\n",
    "        if header_match: # start looking for nets\n",
    "            net_degree = int(header_match[1])\n",
    "            if net_degree < 2:\n",
    "                one_deg_nets += 1\n",
    "                line_num += net_degree\n",
    "                continue\n",
    "            net_name = header_match[2]\n",
    "            current_net = {\"inputs\": []}\n",
    "            for i in range(net_degree):\n",
    "                net_line_match = re.match(net_line_pattern, nets_file_data[line_num])\n",
    "                line_num += 1\n",
    "                assert net_line_match is not None, f\"{net_name} should have {net_degree} pins but found {i}\"\n",
    "                new_pin = Pin(\n",
    "                    id = line_num,\n",
    "                    obj_name = net_line_match[1],\n",
    "                    offset = (float(net_line_match[3]), float(net_line_match[4])),\n",
    "                    )\n",
    "                if net_line_match[2] == \"I\":\n",
    "                    current_net[\"inputs\"].append(new_pin)\n",
    "                else: # this is the source\n",
    "                    if \"output\" not in current_net: \n",
    "                        current_net[\"output\"] = new_pin\n",
    "                    else:\n",
    "                        warn_fn(f\"WARNING: multiple output pins detected in net {net_name}, using first output pin...\")\n",
    "                        current_net[\"inputs\"].append(new_pin)\n",
    "            if \"output\" not in current_net: \n",
    "                warn_fn(f\"WARNING: no output pins detected in {net_name}, using last pin...\")\n",
    "                current_net[\"output\"] = current_net[\"inputs\"].pop()\n",
    "            nets.append(current_net)\n",
    "        else:\n",
    "            assert not nets_started, f\"Expected contiguous block of nets, found {nets_line}, or previous net has more pins than header specifies.\"\n",
    "    print_fn(\"nets and pins after removing one-deg nets\", len(nets), sum([len(n[\"inputs\"])+1 for n in nets]))\n",
    "    print_fn(\"original nets and pins\", one_deg_nets+len(nets), one_deg_nets+sum([len(n[\"inputs\"])+1 for n in nets]))\n",
    "\n",
    "    # Generate per-node outputs\n",
    "    x = torch.tensor(placement_dict[\"positions\"]) # (V, 2)\n",
    "    is_macros = []\n",
    "    cond_x = []\n",
    "    name_index_mapping = {} # used to get object index from object_name\n",
    "    for obj_idx, obj_name in enumerate(placement_dict[\"obj_names\"]): # preserve object order\n",
    "        is_macros.append(node_macro_dict[obj_name])\n",
    "        cond_x.append(node_size_dict[obj_name])\n",
    "        name_index_mapping[obj_name] = obj_idx\n",
    "    is_macros = torch.tensor(is_macros) # (V,) bool\n",
    "    is_ports = torch.zeros_like(is_macros) # for ispd, no ports\n",
    "    cond_x = torch.tensor(cond_x) # (V, 2)\n",
    "    # Generate per-edge outputs\n",
    "    edge_indices = []\n",
    "    edge_attrs = []\n",
    "    edge_pin_ids = []\n",
    "    for net in nets:\n",
    "        net_source_pin = net[\"output\"]\n",
    "        for sink_pin in net[\"inputs\"]:\n",
    "            edge_indices.append(\n",
    "                (name_index_mapping[net_source_pin.obj_name], name_index_mapping[sink_pin.obj_name])\n",
    "            )\n",
    "            edge_attrs.append((*net_source_pin.offset, *sink_pin.offset))\n",
    "            edge_pin_ids.append((net_source_pin.id, sink_pin.id))\n",
    "    edge_index_unique = torch.tensor(edge_indices, dtype=torch.int64) # (E_u, 2)\n",
    "    edge_attr_unique = torch.tensor(edge_attrs) # (E_u, 4)\n",
    "    edge_pin_id_unique = torch.tensor(edge_pin_ids, dtype=torch.int64) # (E_u, 2)\n",
    "\n",
    "    # edge attribute should be measured wrt. bottom left\n",
    "    u_shape = cond_x[edge_index_unique[:,0]]\n",
    "    v_shape = cond_x[edge_index_unique[:,1]]\n",
    "    edge_attr_unique[:,:2] = edge_attr_unique[:,:2] + u_shape/2\n",
    "    edge_attr_unique[:,2:4] = edge_attr_unique[:,2:4] + v_shape/2\n",
    "    \n",
    "    E_u, _ = edge_index_unique.shape\n",
    "    edge_index = flip_stack(edge_index_unique).T\n",
    "    edge_attr = flip_stack(edge_attr_unique.view(E_u, 2, 2)).view(E_u*2, 4)\n",
    "    edge_pin_id = flip_stack(edge_pin_id_unique)\n",
    "\n",
    "    cond = Data(\n",
    "        x = cond_x/SCALING_UNITS,\n",
    "        edge_index = edge_index,\n",
    "        edge_attr = edge_attr/SCALING_UNITS,\n",
    "        edge_pin_id = edge_pin_id,\n",
    "        is_ports = is_ports,\n",
    "        is_macros = is_macros,\n",
    "        name_index_mapping = name_index_mapping,\n",
    "    )\n",
    "    return x/SCALING_UNITS, cond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Simple Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../benchmarks/simple/simple1/simple1.pl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m nets_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../benchmarks/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbenchmark_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcircuit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcircuit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.nets\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m pl_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../benchmarks/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbenchmark_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcircuit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcircuit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m x, cond \u001b[38;5;241m=\u001b[39m \u001b[43mparse_bookshelf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnets_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpl_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m cond[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchip_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m]\n\u001b[0;32m     10\u001b[0m x, cond \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mpreprocess_graph(x, cond, chip_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m))\n",
      "Cell \u001b[1;32mIn[4], line 52\u001b[0m, in \u001b[0;36mparse_bookshelf\u001b[1;34m(nodes_path, nets_path, pl_path, verbose, warn)\u001b[0m\n\u001b[0;32m     49\u001b[0m warn_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mprint\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m warn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Ingest Placement file\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpl_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m pl_file:\n\u001b[0;32m     53\u001b[0m     pl_file_data \u001b[38;5;241m=\u001b[39m pl_file\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m     54\u001b[0m placement_line_pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+(?:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*)?)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+(?:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)?)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*(F?N|S|E|W)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*(/FIXED)?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*$\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hml20\\miniconda3\\envs\\chipdiffusion\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../benchmarks/simple/simple1/simple1.pl'"
     ]
    }
   ],
   "source": [
    "benchmark_name = \"simple\"\n",
    "circuit_name = \"simple1\"\n",
    "nodes_path = f\"../benchmarks/{benchmark_name}/{circuit_name}/{circuit_name}.nodes\"\n",
    "nets_path = f\"../benchmarks/{benchmark_name}/{circuit_name}/{circuit_name}.nets\"\n",
    "pl_path = f\"../benchmarks/{benchmark_name}/{circuit_name}/{circuit_name}.pl\"\n",
    "\n",
    "x, cond = parse_bookshelf(nodes_path, nets_path, pl_path, verbose=True)\n",
    "cond[\"chip_size\"] = [0, 0, 0.1, 0.1]\n",
    "\n",
    "x, cond = utils.preprocess_graph(x, cond, chip_size=(0.1, 0.1))\n",
    "\n",
    "print(x.shape, cond)\n",
    "image1 = utils.visualize_placement(x, cond, plot_pins=True, plot_edges=True, img_size=(1024, 1024))\n",
    "utils.debug_plot_img(image1, name=f\"img_{circuit_name}\")\n",
    "\n",
    "print(\"HPWL\", utils.hpwl_fast(x, cond, normalized_hpwl=False)[1])\n",
    "print(\"Macro HPWL\", utils.macro_hpwl(x, cond, normalized_hpwl=False)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting ISPD Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERTING adaptec1...\n",
      "num objects: 211447\n",
      "num unique objects: 211447\n",
      "num macros: 543\n",
      "nets and pins after removing one-deg nets 219794 942705\n",
      "original nets and pins 221142 944053\n",
      "saved graph to ../datasets/graph/ispd2005-s0\\graph0.pickle and placement to ../datasets/graph/ispd2005-s0\\output0.pickle\n",
      "CONVERTING adaptec2...\n",
      "num objects: 255023\n",
      "num unique objects: 255023\n",
      "num macros: 566\n",
      "nets and pins after removing one-deg nets 260159 1063632\n",
      "original nets and pins 266009 1069482\n",
      "saved graph to ../datasets/graph/ispd2005-s0\\graph1.pickle and placement to ../datasets/graph/ispd2005-s0\\output1.pickle\n",
      "CONVERTING adaptec3...\n",
      "num objects: 451650\n",
      "num unique objects: 451650\n",
      "num macros: 723\n",
      "nets and pins after removing one-deg nets 466295 1874576\n",
      "original nets and pins 466758 1875039\n",
      "saved graph to ../datasets/graph/ispd2005-s0\\graph2.pickle and placement to ../datasets/graph/ispd2005-s0\\output2.pickle\n",
      "CONVERTING adaptec4...\n",
      "num objects: 496045\n",
      "num unique objects: 496045\n",
      "num macros: 1329\n",
      "nets and pins after removing one-deg nets 515304 1911773\n",
      "original nets and pins 515951 1912420\n",
      "saved graph to ../datasets/graph/ispd2005-s0\\graph3.pickle and placement to ../datasets/graph/ispd2005-s0\\output3.pickle\n",
      "CONVERTING bigblue1...\n",
      "num objects: 278164\n",
      "num unique objects: 278164\n",
      "num macros: 560\n",
      "nets and pins after removing one-deg nets 282974 1143186\n",
      "original nets and pins 284479 1144691\n",
      "saved graph to ../datasets/graph/ispd2005-s0\\graph4.pickle and placement to ../datasets/graph/ispd2005-s0\\output4.pickle\n",
      "CONVERTING bigblue2...\n",
      "num objects: 557866\n",
      "num unique objects: 557866\n",
      "num macros: 23084\n",
      "nets and pins after removing one-deg nets 576816 2121863\n",
      "original nets and pins 577235 2122282\n",
      "saved graph to ../datasets/graph/ispd2005-s0\\graph5.pickle and placement to ../datasets/graph/ispd2005-s0\\output5.pickle\n",
      "CONVERTING bigblue3...\n",
      "num objects: 1096812\n",
      "num unique objects: 1096812\n",
      "num macros: 1293\n",
      "nets and pins after removing one-deg nets 1122340 3832388\n",
      "original nets and pins 1123170 3833218\n",
      "saved graph to ../datasets/graph/ispd2005-s0\\graph6.pickle and placement to ../datasets/graph/ispd2005-s0\\output6.pickle\n",
      "CONVERTING bigblue4...\n",
      "num objects: 2177353\n",
      "num unique objects: 2177353\n",
      "num macros: 8170\n",
      "nets and pins after removing one-deg nets 2228903 8899095\n",
      "original nets and pins 2229886 8900078\n",
      "saved graph to ../datasets/graph/ispd2005-s0\\graph7.pickle and placement to ../datasets/graph/ispd2005-s0\\output7.pickle\n"
     ]
    }
   ],
   "source": [
    "ispd_names = [f\"adaptec{i+1}\" for i in range(4)] + [f\"bigblue{i+1}\" for i in range(4)]\n",
    "benchmark_path = \"../benchmarks/ispd2005/\"\n",
    "\n",
    "output_dir = \"../datasets/graph/ispd2005-s0\" # s for self-converted\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for idx, name in enumerate(ispd_names):\n",
    "    print(f\"CONVERTING {name}...\")\n",
    "    nodes_path = os.path.join(benchmark_path, name, f\"{name}.nodes\")\n",
    "    nets_path = os.path.join(benchmark_path, name, f\"{name}.nets\")\n",
    "    pl_path = os.path.join(benchmark_path, name, f\"{name}.pl\")\n",
    "    x, cond = parse_bookshelf(nodes_path, nets_path, pl_path, verbose=True, warn=False)\n",
    "    cond.chip_size = ISPD_CHIP_SIZES[idx]\n",
    "    \n",
    "    cond_path = os.path.join(output_dir, f\"graph{idx}.pickle\")\n",
    "    x_path = os.path.join(output_dir, f\"output{idx}.pickle\")\n",
    "    \n",
    "    with open(cond_path, 'wb') as f:\n",
    "        pickle.dump(cond, f)\n",
    "    with open(x_path, 'wb') as f:\n",
    "        pickle.dump(x, f)\n",
    "    print(f\"saved graph to {cond_path} and placement to {x_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set and val set sizes:  0 8\n",
      "skipping adaptec1, placements/chipformer_validation/ispd2005\\adaptec1\\adaptec1_placed.pl not found\n",
      "skipping adaptec2, placements/chipformer_validation/ispd2005\\adaptec2\\adaptec2_placed.pl not found\n",
      "skipping adaptec3, placements/chipformer_validation/ispd2005\\adaptec3\\adaptec3_placed.pl not found\n",
      "skipping adaptec4, placements/chipformer_validation/ispd2005\\adaptec4\\adaptec4_placed.pl not found\n",
      "skipping bigblue1, placements/chipformer_validation/ispd2005\\bigblue1\\bigblue1_placed.pl not found\n",
      "skipping bigblue2, placements/chipformer_validation/ispd2005\\bigblue2\\bigblue2_placed.pl not found\n",
      "skipping bigblue3, placements/chipformer_validation/ispd2005\\bigblue3\\bigblue3_placed.pl not found\n",
      "skipping bigblue4, placements/chipformer_validation/ispd2005\\bigblue4\\bigblue4_placed.pl not found\n",
      "# objects: []\n",
      "# objects in placement: []\n",
      "# edges: []\n",
      "# macros: []\n",
      "# ports: []\n",
      "# fixed in pl: []\n",
      "mask overlap: []\n"
     ]
    }
   ],
   "source": [
    "task = \"ispd2005-s0\"\n",
    "placement_name = \"chipformer_validation\"\n",
    "# placement_name = None\n",
    "placement_dir = f\"placements/{placement_name}/ispd2005\" if placement_name is not None else None\n",
    "train_set, val_set = utils.load_graph_data_with_config(\n",
    "    task, \n",
    "    train_data_limit = None, \n",
    "    val_data_limit = None,\n",
    "    )\n",
    "\n",
    "print(\"Train set and val set sizes: \", len(train_set), len(val_set))\n",
    "\n",
    "if placement_dir is None:\n",
    "    placed_set = val_set\n",
    "else:\n",
    "    placed_set = load_ispd_placements(placement_dir, val_set)\n",
    "placed_set = [(x.to(device=DEVICE), cond.to(device=DEVICE)) for x, cond in placed_set]\n",
    "mask_overlap = [(cond.fixed_mask & cond.is_macros).sum().cpu().item() for (_, cond) in placed_set]\n",
    "# placed_set = [utils.remove_non_macros(x, cond) for x, cond in placed_set] # This step is not strictly necessary, just speeds up plotting\n",
    "\n",
    "num_objects = [cond.num_nodes for (x, cond) in placed_set]\n",
    "num_placement_objects = [x.shape[0] for (x, cond) in placed_set]\n",
    "num_edges = [cond.num_edges for (x, cond) in placed_set]\n",
    "num_macros = [cond.is_macros.sum().cpu().item() for (x, cond) in placed_set]\n",
    "num_ports = [cond.is_ports.sum().cpu().item() for (x, cond) in placed_set]\n",
    "num_fixed_in_pl = [cond.fixed_mask.sum().cpu().item() for (x, cond) in placed_set] if placement_dir is not None else None\n",
    "print(\"# objects:\", num_objects)\n",
    "print(\"# objects in placement:\", num_placement_objects)\n",
    "print(\"# edges:\", num_edges)\n",
    "print(\"# macros:\", num_macros)\n",
    "print(\"# ports:\", num_ports)\n",
    "print(\"# fixed in pl:\", num_fixed_in_pl)\n",
    "print(\"mask overlap:\", mask_overlap)\n",
    "# print(placed_set[0][1])\n",
    "\n",
    "# plot_set(placed_set, nrows=1, ncols=2, scale_factor=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ispd2005-s0 chipformer_validation\n",
      "Macro HPWL: []\n",
      "HPWL: []\n"
     ]
    }
   ],
   "source": [
    "print(task, placement_name)\n",
    "full_hpwl = eval_set(placed_set, eval_fn=utils.hpwl_fast)\n",
    "hpwl_results = eval_set(placed_set, eval_fn=utils.macro_hpwl)\n",
    "print(\"Macro HPWL:\", hpwl_results)\n",
    "print(\"HPWL:\", full_hpwl)\n",
    "\n",
    "# for i in range(2):\n",
    "#     image1 = utils.visualize_placement(*placed_set[i], plot_pins=True, img_size=(2048, 2048))\n",
    "#     utils.debug_plot_img(image1, name=f\"img{i}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chipdiffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
